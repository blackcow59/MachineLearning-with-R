symptoms = symptoms)
subject
subject[2]
subject[[2]]
subject$temperature
subject[c("temperature", "flu_status")]
pt_data <- data.frame(subject_name, temperature, flu_status, gender, blood, symptoms)
pt_data
str(pt_data)
pt_data$subject_name
pt_data[c("temperture", "flu_status")]
pt_data[c("temperature", "flu_status")]
pt_data[2:3]
pt_data[1,2]
pt_data[c(1,3), c(2,4)]
pt_data[, 1]
pt_data[1,]
pt_data[ , ]
pt_data[c(1,3), c("temperature", "gender")]
pt_data[-2, c(-1, -3, -5, -6)]
pt_data$temp_c <- (pt_data$temperature - 32) *(5 / 9)
pt_data
pt_data[c("temperature", "tmep_C")]
pt_data[c("temperature", "tmep_c")]
pt_data[c("temperature", "temp_c")]
m <- matrix(c(1,2,3), nrow = 2)
m <- matrix(c(1,2,3,4 ), nrow = 2)
m
m <- matrix(c(1,2,3,4), ncol = 2)
m
m <- matrix(c(1,2,3,4,5,6), nrow = 2)
m
m <- matrix(c(1,2,3,4,5,6), ncol = 2)
m
m[1,1]
m[3,2]
m[1,]
m[,1]
save(x, y, z, file = "mydata.RData")
# ls() : 현재 메모리에 있는 데티어 구조의 목록 벡터를 반환한다.
ls()
# rm(객체) : 객체를 제거한다
rm(m, subject)
rm(list = ls())
# 데이터 탐색과 이해
# 데이터 특징과 예시를 탐색하고 데이터를 고유하게 만들어줄 특성을 알아본다.
usedcars <- read.csv("data/usedcars.csv")
usedcars
### 데이터 구조 탐색
str(usedcars)
### 수치 변수 탐색
summary(usedcars$year)
summary(usedcars[c("price", "mileage")])
# 중심 경향 측정 : 평균(mean)과 중앙값(median)
# 퍼짐 측정 : 사분위수와 다섯 숫자 요약 : 최솟값(min), 1사분위수(Q1), 중앙값(median), 3사분위수(Q3), 최댓값(max)
# 범위(range) : 최솟값과 최댓값 사이의 폭
range(usedcars$price)
diff(range(usedcars$price))
# 사분위 범위(IQR)
IQR(usedcars$price)
quantile(usedcars$price)
quantile(usedcars$price, probs = c(0.01, 0.99))
quantile(usedcars$price, seq(from = 0, to = 1, by = 0.2))
# 수치 변수 시각화 : 상자 그림(boxplot)
boxplot(usedcars$price, main = "Boxplot of Used Car Prices")
# 수치 변수 시각화 : 상자 그림(boxplot)
boxplot(usedcars$price, main = "Boxplot of Used Car Prices", ylab = "Price ($)")
boxplot(usedcars$mileage, main = "Boxplot of Used car Mileage", ylab = "Odometer (mi.)")
# 수치 변수 시각화 : 히스토그램(hist)
hist(usedcars$price, main = "Histogram of Used Car Prices", xlab = "Price ($)")
hist(usedcars$mileage, main = "Histogram of Used Car Mileage", xlab = "Odometer (mi.)")
hist(usedcars$mileage, main = "Histogram of Used Car Mileage", xlab = "Odometer (mi.)", breaks = 10)
hist(usedcars$mileage, main = "Histogram of Used Car Mileage", xlab = "Odometer (mi.)", breaks = 20)
hist(usedcars$mileage, main = "Histogram of Used Car Mileage", xlab = "Odometer (mi.)", breaks = 10)
hist(usedcars$mileage, main = "Histogram of Used Car Mileage", xlab = "Odometer (mi.)", breaks = c(5000, 10000, 15000, 20000))
hist(usedcars$mileage, main = "Histogram of Used Car Mileage", xlab = "Odometer (mi.)", breaks = c(5000, 10000, 15000, 20000, ...))
hist(usedcars$mileage, main = "Histogram of Used Car Mileage", xlab = "Odometer (mi.)", breaks = c(50000, 100000, 150000, 200000))
hist(usedcars$mileage, main = "Histogram of Used Car Mileage", xlab = "Odometer (mi.)", breaks = c(0, 50000, 100000, 150000, 200000))
# 수치 변수 시각화 : 히스토그램(hist)
hist(usedcars$price, main = "Histogram of Used Car Prices", xlab = "Price ($)")
# 퍼짐 측정 : 분산(var)과 표준편차(sd)
var(usedcars$price)
sd(usedcars$price)
var(usedcars$mileage)
sd(usedcars$mileage)
table(usedcars$year)
table(usedcars$model)
table(usedcars$color)
model_table <- table(usedcars$model)
prop.table(model_table)
round(prop.table(model_table), digits = 1)
color_table <- table(usedcars$color)
color_pct <- prop.table(color_table) *100
round(color_pct, digits = 1)
# 관계 시각화 : 산포도(산점도)
# 산포도(산점도) : 수치 특징 사이의 이변량 관계를 시각화하는 다이어그램
plot(price ~ mileage, data = usedcars,
main = "Scatterplot of Price vs. Mileage",
xlab = "Used Car Odometer (mi.)",
ylab = "Used Car Price ($)")
# 관계 관찰 : 이원 교차표
install.packages("gmodels")
# 관계 관찰 : 이원 교차표
# install.packages("gmodels")
require(gmodels)
usedcars$conservative <- usedcars$color %in% c("Black", "Gray", "Sliver", "White")
table(usedcars$conservative)
usedcars$conservative <- usedcars$color %in% c("Black", "Gray", "Silver", "White")
table(usedcars$conservative)
CrossTable(x = usedcars$model, y = usedcars$conservative)
CrossTable(x = usedcars$model, y = usedcars$conservative, chisq = T)
wbcd <- read.csv("data/wisc_bc_data.csv")
str(wbcd)
wbcd <- read.csv("data/wisc_bc_data.csv")
str(wbcd)
# id 변수는 단순히 데이터 내에서의 환자 아이디이기 때문에 유용한 정보를 제공하지 않으며 모델에서 제외할 필요가 있다.
# 머신러닝 방법에 상관없이 id변수는 항상 제외시켜야 한다.
wbcd <- wbcd[-1]
# diagnosis는 예측하려는 결과이므로 특별히 관심이 있다.
# 이 특징은 예시가 양성 종양인지 음성 종양인지 여부를 나타낸다
table(wbcd$diagnosis)
# 많은 R 머신러닝 분류기는 목표 특징이 팩터로 코딩돼야만 한다.
# 따라서 diagnosis변수를 다시 코드화 해야한다.
# 또한 labels파라미터를 이용해 "B"와 "M" 값에 유용한 정보를 주는 레이블을 제공할 것이다.
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
# 설명을 위해 남은 특징 중 세 개만 자세히 살펴보자.
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
## 변환 : 수치 데이터 정규화
normalize <- function(x) {
return((x - mean(x)) / (max(x) - min(x)))
}
normalize(c(1,2,3,4,5))
return((x - min(x)) / (max(x) - min(x)))
## 변환 : 수치 데이터 정규화
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1,2,3,4,5))
normalize(c(10, 20, 30, 40, 50))
# lapply(list, function) : 리스트를 취해 각 리스트 항목에 지정된 함수를 적용한다.
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n)
## 데이터 준비 : 훈련 및 테스트 데이터셋 생성
# 학습자가 레이블이 없는 데이터의 잡합에 대해 얼마나 잘 수행되는가?
# k-NN 모델을 구축하고자 사용되는 훈련 데이터셋과
# 모델의 예측 정확도를 평가하고자 사용되는 테스트 데이터셋
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
# k-NN모델을 훈련하고자 diagnosis를 팩터 벡터에 저장하고 훈려 데이터셋과 테스트 데이터셋으로 분리한다.
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
# 테스트 인스턴스를 분류하고자 분류를 위한 기본 R 함수를제공하는 class 패키지의 k-NN 구현을 사용할 것이다.
install.packages("class")
# 테스트 인스턴스를 분류하고자 분류를 위한 기본 R 함수를제공하는 class 패키지의 k-NN 구현을 사용할 것이다.
# install.packages("class")
require(class)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 21)
# gmodels 패키지의 CrossTable() 함수 이용
require(gmodels)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = F)
# 벡터를 표준화 하고자 R 내장함수 scale()을 이용해 기본 방식인 z-점수 표준화 방식으로 값을 재조정한다.
# scale() 함수는 데이터 프렝미에 직접 적용할 수 있으므로 lapply()함수를 사용할 필요가 없다.
wbcd_z <- as.data.frame(scale(wbcd[-1]))
summary(wbcd_z)
wbcd_train_z <- wbcd_z[1:469, ]
wbcd_test_z <- wbdcd_z[470:569, ]
wbcd_test_z <- wbcd_z[470:569, ]
wbcd_train_labels_z <- wbcd[1:469, 1]
wbcd_test_labels_z <- wbcd[470:569, 1]
wbcd_test_pred_z <- knn(train = wbcd_train_z, test = wbcd_test_z,
cl = wbcd_train_labels_z, k = 21)
CrossTable(x = wbcd_test_labels_z, y = wbcd_test_pred_z,
prop.chisq = F)
## k의 대체 값 테스트
wbcd_test_pred_21 <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred_21, prop.chisq = F)
wbcd_test_pred_1 <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 1)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred_1, prop.chisq = F)
# k = 5
wbcd_test_pred_5 <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 5)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred_5, prop.chisq = F)
# k = 11
wbcd_test_pred_11 <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 11)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred_11, prop.chisq = F)
# k = 15
wbcd_test_pred_15 <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 15)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred_15, prop.chisq = F)
# k = 27
wbcd_test_pred_27 <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 27)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred_27, prop.chisq = F)
source('C:/r-project/MachineLearning/script/04. 확룰적 학습-나이브 베이즈 분류.R', encoding = 'UTF-8')
sms_raw <- read.csv("data/sms_spam.csv")
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw)
str(sms_raw$type)
sms_raw <- read.csv("data/sms_spam.csv")
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
sms_raw$type
table(sms_raw$type)
view(sms_raw)
View(sms_raw)
sms_raw$type[1072]
sms_raw$text[1072]
sms_raw$text[1072]<- sms_raw$type[1072]
sms_raw$text[1072]
sms_raw$type[1072]
sms_raw <- read.csv("data/sms_spam.csv")
sms_raw$text[1072]<- sms_raw$type[1072]
sms_raw$text[1072]
sms_raw$type[1072]
sms_raw$type[1072] <- "ham"
sms_raw$type[1072]
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
View(sms_raw)
# SMS 메시지는 단어, 공백, 숫자, 구두점으로 구성된 텍스트 문자열이다. 이런 종류의 복잡한 데이터를 다루려면 많은 생각과 노력이 필요하다.
# 숫자와 구두점 제거 방법, and, but, or 같은 관심 없는 단어의 처리 방법과 문장을 개별 단어로 나누는 방법을 고려할 필요가 있다.
# 다행이도 이 기능은 R의 tm이라는 텍스트 마이닝 패키지로 제공하고 있다.
install.packages("tm")
require(tm)
# 코퍼스를 생성하려면 휘발성 코퍼스를 참조하는 tm 패키지의 VCorpus()함수를 사용한다.
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
# tm 코퍼스는 근본적으로 복합 리스트이기 때문에 코퍼스에서 리스트 연산을 사용해 문서를 선택할 수 있다.
inspect(sms_corpus[1:@])
# tm 코퍼스는 근본적으로 복합 리스트이기 때문에 코퍼스에서 리스트 연산을 사용해 문서를 선택할 수 있다.
inspect(sms_corpus[1:2])
as.character(sms_corpus[[1]])
# 여러 문서를 보려면 sms_corpus 객체의 여러 항목에 as.character()를 적용해야 한다.
lapply(sms_corpus[1:2], as.character)
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
?tm_map
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower()))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower(sms_corpus)))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower)
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, tolower)
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, tolower)
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
as.character(sms_corpus_clean[[1]])
# 코퍼스를 생성하려면 휘발성 코퍼스를 참조하는 tm 패키지의 VCorpus()함수를 사용한다.
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
sms_corpus_clean <- tm_map(sms_corpus, removeNumbers)
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, tolower)
(sms_corpus)
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, tolower)
sms_corpus_clean <- tm_map(sms_corpus, removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus_clean, tolower)
as.character(sms_corpus[[1]])
sms_corpus_clean <- tm_map(sms_corpus, removeNumbers)
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
View(sms_corpus)
sms_raw <- read.csv("data/sms_spam.csv")
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
sms_raw <- read.csv("data/sms_spam.csv", stringsAsFactors = T)
str(sms_raw)
sms_raw <- read.csv("data/sms_spam.csv", stringsAsFactors = T)
str(sms_raw)
sms_raw <- read.csv("data/sms_spam.csv")
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
require(tm)
# 코퍼스를 생성하려면 휘발성 코퍼스를 참조하는 tm 패키지의 VCorpus()함수를 사용한다.
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
# inspect() 함수는 결과의 요약을 보여준다.
inspect(sms_corpus[1:2])
# as.character() 함수를 이용해 실제 메시지 텍스트를 볼 수 있다.
as.character(sms_corpus[[1]])
# 여러 문서를 보려면 sms_corpus 객체의 여러 항목에 as.character()를 적용해야 한다.
lapply(sms_corpus[1:2], as.character)
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, tolower)
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
# 첫 번째 변환은 소문자만 사용하도록 메시지를 표준화하는 것이다.
# 이를 위해 tolower() 함수를 이용한다.
# 이 함수를 코퍼스에 적용하려면 tm 래퍼 함수 content_transformer()를 사용해서 tolower()가 코퍼스에 접근하는 데 사용되는 변환 함수로 취급되게 해야한다.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
gsub(pattern = "[^0-9a-zA-Zㄱ-힣]", replacement = "", sms_raw)
gsub(pattern = "[^0-9a-zA-Zㄱ-힣]", replacement = "", sms_raw$text)
gsub(pattern = "[0-9a-zA-Zㄱ-힣]", replacement = "", sms_raw$text)
View(sms_raw$text[105])
sms_raw$text[105]
as.character(sms_corpus[[1]])
# SMS 메시지에서 숫자를 제거해 정리를 계속해보자.
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
as.character(sms_corpus_clean[[1]])
# 다음 작업은 to, and, but, or와 같이 채우기 위한 단어를 SMS 메시지에서 제거하는 것이다.
# 이런 용어를 불용어라고 하며, 일반적으로 텍스트 마이닝을 하기 전에 제거한다.
sms_corpus_clean <- tm_map(sms_corpus_clean,
removeWords, stopwords())
# 정리 과정을 계속 진행하고자 removePunctuation() 변환으로 문자 메시지에서 구두점을 제거할 수 있다.
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
# removePunctuation()의 디폴트 동작을 피하려면 간단히 구두점 문자를 제거하는 대신 대체하는 사용자 정의 함수를 생성한다.
replacePunctuation <- function(x) {
gsub("[[:punct:]]+", " ", x)
}
# tm 패키지는 SnowballC 패키지와 통합돼 형태소 분석 기능을 제공한다.
install.packages("SnowballC")
require(SnowballC)
# SnowballC 패키지는 wordStem() 함수를 제공해 문자 벡터에 대해 어근 형태의 동일한 벡터를 반환한다.
wordStem(c("learn", "learned", "learning", "learns"))
# wordStem() 함수를 텍스트 문서의 전체 코퍼스에 적용하고자 tm 패키지는 stemDocument() 변환을 제공한다.
# 이전처럼 tm_map() 함수를 이용해 stemDocument()변환을 코퍼스에 정확히 적용한다.
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
# 숫자, 불용어, 구두점을 제거하고 형태소 분석을 실행하고 나면 문자 메시지는 지금은 제거된 조각을 분리했던 빈칸과 함께 남는다.
# 따라서 텍스트 정리 과정의 최종단계는 stripWhitespace() 변환을 이용해서 추가 여백을 제거하는 것이다.
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
as.character(sms_corpus[1:3])
# 코퍼스를 생성하려면 휘발성 코퍼스를 참조하는 tm 패키지의 VCorpus()함수를 사용한다.
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
# inspect() 함수는 결과의 요약을 보여준다.
inspect(sms_corpus[1:2])
# as.character() 함수를 이용해 실제 메시지 텍스트를 볼 수 있다.
as.character(sms_corpus[[1]])
as.character(sms_corpus[1:3])
as.character(sms_corpus[1])
as.character(sms_corpus[[1:3]])
as.character(sms_corpus[[1]])
lapply(sms_corpus[1:3], as.character)
lapply(sms_corpus_clean[1:3], as.character)
# tm 패키지의 DocumentTermMatrix() 함수는 코퍼스를 가져와 문서 용어 행렬이라고 하는 데이터 구조를 만든다.
# 이때 행렬의 행은 문서(SMS 메시지)를 나타내고, 열은 용어(단어)를 나타낸다.
# 행렬에서 각 셀은 행이 표현하고 있는 문서에서 열이 표현하고 있는 단어가 출현하는 횟수를 저장한다.
# 이 데이터 구조는 희소행렬(대다수의 셀이 0으로 채워져 있다.)이라고 불린다.
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
# 사전처리를 수행하지 않았다면 기본 설정을 재지정하는 control 파라미터 옵션 목록을 제공해 사전 처리를 여기서 할 수 있다.
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(
tolower = T,
removeNumbers = T,
stopwords = T,
removePunctuation = T,
stemming = T
))
# 이 명령은 앞에서 했던 것과 같은 순서로 SMS 코퍼스에 동일한 사전 처리 단계를 적용한다.
# 하지만 sms_dtm을 sms_dtm2와 비교하면 행렬의 용어 개수에 약간의 차이를 확인할 수 있다.
sms_dtm
sms_dtm2
# 데이터를 훈련용 75%와 테스트용25% 두 부분으로 분리하려고 한다.
# SMS 메시지는 임의의 순서로 정렬돼 있으므로, 단순히 처음 4,169개를 훈련용으로 가져오고 나머지 1,390개를 테스트용으로 남겨둔다.
sms_dtm_train <- sms_dtm[1:4169,]
sms_dtm_test <- sms_dtm[4170:5559, ]
# 편의상 훈련과 테스트 데이터 프레임의 행별 레이블을 갖는 벡터를 각각 저장해두는 것이 좋다.
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
# 이 부분집합이 SMS 데이터 전체 집합을 대표하는지를 확인하고자 훈련 데이터 프레임과 테스트 데이터 프레임의 스팸 비율을 비교해보자.
prop.table(tabe(sms_train_labels))
# 이 부분집합이 SMS 데이터 전체 집합을 대표하는지를 확인하고자 훈련 데이터 프레임과 테스트 데이터 프레임의 스팸 비율을 비교해보자.
prop.table(table(sms_train_labels))
prop.talbe(table(sms_test_labels))
prop.tablbe(table(sms_test_labels))
prop.table(table(sms_test_labels))
# 단어 구름은 텍스트 데이터에서 단어가 나타나는 빈도를 시각적으로 보여주는 방법이다.
# 텍스트에서 더 자주 나타나는 단어는 더 큰 폰트로 보여주고, 덜 일반적인 용어는 더 작은 폰트로 보여준다.
# 스팸과 햄의 구름을 비교하는 것은 나이브 베이즈 스팸 필터의 성공 여부를 판단하는데 도움이 되므로, SMS 메시지의 단어 유형을 시각화하는데 이 함수를 사용할 것이다.
install.packages("wordcloud")
# 단어 구름은 텍스트 데이터에서 단어가 나타나는 빈도를 시각적으로 보여주는 방법이다.
# 텍스트에서 더 자주 나타나는 단어는 더 큰 폰트로 보여주고, 덜 일반적인 용어는 더 작은 폰트로 보여준다.
# 스팸과 햄의 구름을 비교하는 것은 나이브 베이즈 스팸 필터의 성공 여부를 판단하는데 도움이 되므로, SMS 메시지의 단어 유형을 시각화하는데 이 함수를 사용할 것이다.
# install.packages("wordcloud")
require(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = F)
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
par(mfrow = c(1,2))
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
# 자주 나타나는 단어를 찾으려면 tm 패키지에서 findFreqTerms() 함수를 사용하면 된다.
# 이 함수는 DTM을 받아 최소 횟수만큼 나타나는 단어를 포함하는 문자 벡터를 반환한다.
findFreqTerms(sms_dtm_train, 5)
sms_freq_word <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_word)
# 이제 DTM을 필터링해 벡터에 자주 나타나는 용어만 포함하게 할 필요가 있다.
# 앞서 했듯이 DTM의 특정 부분을 요청하려면 데이터 프레임 스타일 연산을 사용하되 열 이름이 DTM에 포함된 단어를 따른다는 점을 주목해야 한다.
sms_dtm_freq_train <- sms_dtm_train[, sms_freq_word]
sms_dtm_freq_test <- sms_dtm_test[, sms_freq_word]
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
# apply() 함수는 행렬의 각 행이나 열에 대해 임의의 함수가 호출되게 한다.
# 그리고 행이나 열을 명시하고자 MARGIN 파라미터를 사용한다.(1 = 행, 2 = 열)
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test <- applt(sms_dtm_freq_test, MARGIN = 2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
sms_train
sms_test
sms_dtm_freq_train
# 사용할 나이브 베이즈 구현은 e1071 패키지에 있다.
install.packages("e1071")
# 사용할 나이브 베이즈 구현은 e1071 패키지에 있다.
# install.packages("e1071")
require(e1071)
# 예제 :
sms_classfier <- naiveBayes(sms_train, sms_type)
sms_prediction <- predict(sms_classfier, sms_test)
# sms_train 행렬에 있는 모델을 구축하려면 다음 명령을 이용한다.
sms_classfier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classfier, sms_test)
# 예측을 실제 값과 비교하고자 이전에 사용했던 gmodels 모델 패키지에 있는 CrossTable() 함수를 사용한다.
# dnn 파라미터로 행과 열을 다음 코드처럼 다시 레이블을 지정할 것이다.
require(gmodels)
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = F, prop.c = F,
prop.r = F, dnn = c('predicted', 'actual'))
# 이전과 같이 나이브 베이즈 모델을 구축하지만, 이번에는 laplace = 1로 설정한다.
sms_classfier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)
sms_test_pred2 <- predict(sms_classfier2, sms_test)
CrossTable(sms_test_pred2, sms_test_labels,
prop.chisq = F, prop.c = F, prop.r = F,
dnn = c('predicted', 'actual'))
